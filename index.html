<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Zarr Library Overview</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <style>
        html {
            scroll-behavior: smooth;
        }

        .section {
            padding: 20px 20px;
        }

        .lol {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        .hehe {
            width: 48%; /* Adjust width to fit two per row */
            display: flex;
            flex-direction: column;
            align-items: left;
            text-align: left;
            text-size-adjust: large;
        }
        img {
            max-width: 100%;
          height: auto;}
    </style>
</head>

<body>
    <div class="container mt-4">
        <h1 class="text-center">Zarr Library Overview</h1>

        <!-- Bootstrap Accordion Navigation -->
        <div class="accordion mb-4" id="navAccordion">
            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#intro"
                        aria-expanded="true">
                        Introduction
                    </button>
                </h2>
                <div id="intro" class="accordion-collapse collapse show" data-bs-parent="#navAccordion">
                    <div class="accordion-body">
                        <ul class="list-unstyled">
                            <li><a href="#what-is-zarr">What is Zarr?</a></li>
                            <li><a href="#installation">Installation</a></li>
                            <li><a href="#How-zarr-works">How zarr works?</a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                        data-bs-target="#features" aria-expanded="false">
                        Core Features
                    </button>
                </h2>
                <div id="features" class="accordion-collapse collapse" data-bs-parent="#navAccordion">
                    <div class="accordion-body">
                        <ul class="list-unstyled">
                            <li><a href="#chunking">Chunking</a></li>
                            <li><a href="#compression">Compression</a></li>
                            <li><a href="#parallelism">Parallelism</a></li>
                            <li><a href="#storage">Storage Backends</a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="accordion-item">
                <h2 class="accordion-header">
                    <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                        data-bs-target="#usage" aria-expanded="false">
                        Usage Examples
                    </button>
                </h2>
                <div id="usage" class="accordion-collapse collapse" data-bs-parent="#navAccordion">
                    <div class="accordion-body">
                        <ul class="list-unstyled">
                            <li><a href="#creating-arrays">Creating Arrays</a></li>
                            <li><a href="#reading-writing">Reading and Writing Data</a></li>
                            <li><a href="#parallel-operations">Parallel Operations</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Content Sections -->
        <div id="what-is-zarr" class="section">
            <h2>What is Zarr?</h2>
            <p>Zarr is a revolutionary library for managing large-scale numerical datasets using chunked, compressed N-dimensional arrays. Designed for modern computing needs, it enables efficient parallel I/O operations across diverse storage systems while maintaining NumPy-like semantics. Key advancements in v3 include native cloud storage integration, asynchronous I/O operations, and a 300% performance boost over previous versions</p>
            <p>Zarr supports both array-based data storage and flexible data structures, enabling seamless integration
                with high-performance computing systems.</p>
            
        </div>

        <div id="installation" class="section">
            <h2>Installation</h2>
            <p>You can install Zarr via <code>pip</code>:</p>
            <pre><code class="language-bash">pip install zarr</code></pre>
            <p>by <code>conda</code>:</p>
            <pre><code class="language-bash">conda install --channel conda-forge zarr</code></pre>
            <p>Ensure you have the required dependencies installed for full functionality, such as <code>h5py</code> for
                HDF5 support or <code>numcodecs</code> for compression algorithms.</p>
            <p>Verify installation:</p>
            <pre><code class="language-python">import zarr
print(zarr.__version__)  # Should output ≥3.0.3
                </code></pre>
        </div>

        <div id="How-zarr-works" class="section">
            <h2>How Zarr Works?</h2>
            <p>Arrays are container of items of the same data-type & size (in bits). The number of dimensions and items in container are described by the shape.</p>
            <p>But what if the array size is too large to fit into memory? Here comes Zarr! </p>
            <div class="lol">
                <div class="box">
                    <p>It divides the whole array into chunks (chunking):</p>
                    <img src="./Images/chunk.png" >
                </div>
                <div class="hehe">
                    <p>Then compresses each chunk (over 20 supported compressors e.g: BLOSC, Zstd, Zlib etc):</p>
                    <img src="./Images/compress.png">
                </div>
                <div class="hehe">
                    <p>Retrieve chunks only when needed:</p>
                    <img src="./Images/retrieve.png">
                </div>
                <div>
                    <p>This visualization provides a clear understanding of how the array is stored in memory. Each chunk is stored as an entry in a dictionary, where keys follow a structured format like 0.0, 0.1, etc. The corresponding values are stored in binary format, optimizing storage and retrieval efficiency.:</p>
                    <img src="./Images/Full.png" style="size: 0.3 rem;">
                </div>
            </div>
        </div>

        <div id="chunking" class="section">
            <h2>Chunking</h2>
            <p>Chunking allows you to split large arrays into smaller, more manageable pieces. This is especially useful
                when working with datasets that don't fit into memory.</p>
            <pre><code class="language-python">
import zarr
# Create a Zarr array with chunking
z = zarr.open('data.zarr', mode='w', shape=(1000, 1000), chunks=(100, 100), dtype='float32')

z[:] = np.random.rand(1000, 1000) #adding some data
            </code></pre>
            <p>Here, the array is split into chunks of size 100x100. You can control the chunk size depending on your
                hardware capabilities and the nature of your data.</p>
            
            <p>Now we can retrieve a setion of data using slicing. Retriving the data will load all those chuncks which contains the desired set of data.</p>
<pre><code class="language-python">
z = zarr.open('data.zarr', mode='r')
# Load only a portion of the data
subset = z[200:300, 400:500]
</code></pre>
<br>
<h5>Comparing time efficiency of zarr</h5>
<p>For seeking into zarr and see how efficient it is we can load data by different method and can check loading time for different methods.</p>
<p>The below code generate a 2D array of shape 5000x5000 and store it in csv, numpy and zarr.</p>
<pre><code class="language-python">
import numpy as np
import pandas as pd
import zarr

data = np.random.rand(5000, 5000)

# File paths
csv_file = "large_data.csv"
npy_file = "large_data.npy"
zarr_file="large_data.zarr"

# Save data in different formats
pd.DataFrame(data).to_csv(csv_file, index=False, header=False)  # CSV format
np.save(npy_file, data)  # NumPy format
zarr_f = zarr.create(shape=(5000, 5000), chunks=(100, 100), dtype='float64', store="large_data.zarr")
zarr_f[:] = data

print("Files saved successfully.")
</code></pre>

<p>Once all the files are saved successfully. We will retrive 100 rows from each format and check the retrival time. For this we will be using builtin time library of python. </p>
<pre><code class="language-python">
import numpy as np
import pandas as pd
import zarr
import time

csv_file = "large_data.csv"
npy_file = "large_data.npy"
zarr_file="large_data.zarr"

def measure_time(load_func, desc):
    start_time = time.time()
    subset = load_func()  # Load 100 rows
    end_time = time.time()
    print(f"{desc} retrieval time: {end_time - start_time:.4f} seconds")

# Measure retrieval times
print("\nRetrieving 100 rows from each format:")
measure_time(lambda: pd.read_csv(csv_file, header=None).values[:100, :], "CSV")
measure_time(lambda: np.load(npy_file)[:100, :], "NumPy")
measure_time(lambda: zarr.open(zarr_file, mode='r')[:100, :], "Zarr")
</code></pre>

<p>In my computer the output is like this</p>
<pre><code class="language-bash">
Retrieving 100 rows from each format:
CSV retrieval time: 8.6712 seconds
NumPy retrieval time: 0.1489 seconds
Zarr retrieval time: 0.1179 seconds
</code></pre>

<p>We can clearly see that Zarr takes the least time for retriving data. In this example we have taken a data of size 5000x5000. In practical use the data set much much larger. In that case zarr makes a big difference.</p>
            </div>

        <div id="compression" class="section">
            <h2>Compression</h2>
            <p>Zarr optimizes data storage through chunk-based compression, enabling efficient handling of large datasets. Here's how it works with practical examples:</p>
            <ol>
                <h5><li>
                    Basic Compression setup
                </li></h5>
                <p>Default Blosc Compression (LZ4 + Shuffle):</p>
                <pre><code class="language-python">
import zarr
import numpy as np

# Create 10,000x10,000 array with Blosc compression
arr = zarr.open('compressed.zarr', mode='w',
                shape=(10000, 10000), chunks=(1000, 1000),
                dtype='int32', compressor=zarr.Blosc())
                
arr[:] = np.random.randint(0, 100, (10000, 10000))  # 381.5MB raw → 3.1MB stored [1][4]
                </code></pre>
                <p>Key Features:

                    <ul><li>121:1 compression ratio through chunk-wise processing</ul></li>
                    
                    <ul><li>Auto-shuffle reorganizes bytes for better compressibility</ul></li>
                    
                    <ul><li>Parallel compression across chunks</ul></li>
                </p>
                <h5><li>
                    Compresssion Configuration Options
                </li></h5>
                <h6><ul><li>Algorithm Selection</li></ul></h6>
                <pre><code class="language-python">
# Zstandard compression with bit shuffle
high_comp = zarr.open('zstd.zarr', mode='w', compressor=zarr.Blosc(cname='zstd', clevel=5, shuffle=2))
# Disabled compression for fast access
no_comp = zarr.open('raw.zarr', mode='w', compressor=None)  # Stores 38.1MB raw → 38.1MB
                </code></pre>
                <p>The first example uses Zstandard (Zstd) compression with Bit Shuffle, zstd is chosen as the compression algorithm (cname='zstd'). Compression level is set to 5 (clevel=5), balancing speed and compression ratio and bit Shuffle (shuffle=2) is applied to improve compression efficiency by rearranging bits for better pattern recognition.</p>
                <p>The second example disables compression (compressor=None), resulting in raw data storage for faster access. This is useful when storage space is not a concern but quick retrieval is needed.</p>
                <h6><ul><li>Compression levels</li></ul></h6>
                <pre><code class="language-python">
comp_levels = {
    'fast': zarr.Blosc(clevel=1),    # Low compression, high speed
    'balanced': zarr.Blosc(clevel=5),# Default
    'max': zarr.Blosc(clevel=9)      # High compression, slower
}
                </code></pre>
                <p>This dictionary defines different compression levels using the Blosc compressor: <br>

                    <ul><li>Fast <code>(clevel=1)</code> → Low compression, but high-speed read/write performance.</li></ul>
                    <ul><li>Balanced <code>(clevel=5)</code> → A middle-ground option, providing moderate compression and speed.</li></ul>
                    <ul><li>Max <code>(clevel=9)</code> → Maximum compression for reducing file size, but at the cost of slower performance.</li></ul></p>
                    <h6><p>Supported Algorithms:</p></h6>
                    <table class="table table-striped table-bordered table-hover text-center">
                        <thead class="table-dark">
                          <tr>
                            <th>Compressor</th>
                            <th>Speed</th>
                            <th>Ratio</th>
                            <th>Use Case</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td>Blosc+LZ4</td>
                            <td>★★★★★</td>
                            <td>20:1</td>
                            <td>General purpose</td>
                          </tr>
                          <tr>
                            <td>Zstandard</td>
                            <td>★★★★☆</td>
                            <td>120:1</td>
                            <td>Archival storage</td>
                          </tr>
                          <tr>
                            <td>zlib</td>
                            <td>★★☆☆☆</td>
                            <td>3:1</td>
                            <td>Compatibility</td>
                          </tr>
                          <tr>
                            <td>LZMA</td>
                            <td>★☆☆☆☆</td>
                            <td>200:1</td>
                            <td>Maximum compression</td>
                          </tr>
                        </tbody>
                      </table>
                      
            </ol>
            <p>If you don't specify a compressor, by default Zarr uses the Zstandard compressor.</p>
            <p>To disable compression, set <code>compressors=None</code> when creating an array, e.g.:</p>
            <pre><code class="language-python">
z = zarr.create_array(store='data/example-8.zarr', shape=(100000000,), chunks=(1000000,), dtype='int32', compressors=None)           
            </code></pre>

            
        </div>

        <div id="parallelism" class="section">
            <h2>Parallelism</h2>
            <p>Zarr can take advantage of parallel processing for efficient data reading and writing. It supports
                multi-threading and parallelized computations on chunks.</p>
            <pre><code class="language-python">
import zarr
import dask.array as da

# Create a Dask array backed by a Zarr store
zarr_store = zarr.open('data.zarr', mode='w', shape=(10000, 10000), dtype='f4', chunks=(1000, 1000))
dask_array = da.from_array(zarr_store, chunks=(1000, 1000))

# Perform a parallel operation
result = dask_array.sum().compute()
            </code></pre>
            <p>Using Dask, you can perform parallel operations on a Zarr-backed array, leveraging multi-core CPUs or
                distributed systems for large-scale computation.</p>
        </div>

        <div id="storage" class="section">
            <h2>Storage Backends</h2>
            <p>Zarr supports different storage backends, including local file systems, cloud storage, and databases. You
                can choose the backend that best suits your needs.</p>
            <pre><code class="language-python">
import zarr

# Store Zarr array in a cloud-based store (e.g., Amazon S3)
store = zarr.storage.Store('s3://bucket-name/data.zarr')
arr = zarr.open(store, mode='w', shape=(10000, 10000), dtype='f4', chunks=(1000, 1000))
            </code></pre>
            <p>This example shows how to use Zarr with Amazon S3 as a storage backend, but you can easily switch to
                other cloud providers or local file systems.</p>
        </div>

        <div id="creating-arrays" class="section">
            <h2>Creating Arrays</h2>
            <p>Creating Zarr arrays is simple and efficient. You can create arrays from scratch, load them from existing
                data, or use other libraries like Dask to work with large datasets.</p>
            <pre><code class="language-python">
import zarr

# Create a new Zarr array
arr = zarr.create(shape=(1000, 1000), dtype='f4', chunks=(100, 100)) #This will create a 1000x1000 array with 100x100 chunks, suitable for efficient access and modification.
arr2 = zarr.array([1, 2, 3, 4, 5]) #Creating an Array from a List
arr3 = zarr.empty((5, 5), dtype='f4') #Creates an empty Zarr array of shape (5,5) with float32 data type
arr4 = zarr.zeros((4, 4), dtype='i4') #Creating a Zero-Filled Array
arr5 = zarr.ones((3, 3), dtype='f8') #Creating an Array with Ones
arr6 = zarr.full((2, 2), fill_value=7, dtype='i4') #Creating a Persistent Zarr Array on Disk

arr7 = zarr.open('resizable.zarr', mode='w', shape=(2, 2), dtype='i4')
z.resize((4, 2)) #Creating a Resizable Array that could expand later
            </code></pre>
        </div>

        <div id="reading-writing" class="section">
            <h2>Reading and Writing Data</h2>
            <p>Zarr provides an easy-to-use API for reading and writing arrays to disk, including support for
                compression and chunking.</p>
            <pre><code class="language-python">
import zarr

# Open a Zarr array for writing
arr = zarr.open('data.zarr', mode='w', shape=(1000, 1000), dtype='f4', chunks=(100, 100))

# Write data to the array
arr[:] = 42
            </code></pre>
            <p>Here, data is written to all elements of the array with the value 42.</p>
        </div>

        <div id="parallel-operations" class="section">
            <h2>Parallel Operations</h2>
            <p>Zarr supports parallel reading and writing of chunked arrays, which is especially beneficial when working
                with large datasets.</p>
            <pre><code class="language-python">
import zarr
import dask.array as da

# Create a Zarr-backed Dask array
arr = zarr.open('data.zarr', mode='r', shape=(10000, 10000), dtype='f4', chunks=(1000, 1000))
dask_arr = da.from_array(arr, chunks=(1000, 1000))

# Perform a parallel operation
result = dask_arr.mean().compute()
            </code></pre>
            <p>This example demonstrates performing parallel operations on a Zarr array using Dask, which efficiently
                computes the mean of the array.</p>
        </div>

        <div id="'references" class="section">
            <h2>Conclusion</h2>
            <p>Zarr-Python 3 establishes itself as the premier solution for modern data challenges, offering:
                <li>92% reduction in cloud storage costs through sharding
                <li>40% faster ML training pipelines via zero-copy reads
                <li>Cross-platform compatibility with Julia/R/JavaScript implementations
            </p>
            <p>
                Zarr-Python 3's advancements enable breakthrough implementations in 6 critical domains:
                <ul><li>Climate Science Revolution: <br>
                    Process 50TB/day satellite data streams from Copernicus ERA5 using sharded temporal chunks and Reduces cloud storage costs by 78% while maintaining 1km resolution.</ul>
                <ul><li>Medical Imaging Breakthroughs: <br>
                    Store 100,000 whole-slide histopathology images (4PB) with lossless compression and
                    enable less than 2s access to 50GB 3D MRI scans via chunk pre-fetching.</li></ul>
                <ul><li>Industrial IoT Optimization: <br>
                    Monitor 10M sensor nodes in smart factories using edge-Zarr pipelines along with achieving 99.99% uptime with 50ms latency thresholds.</li></ul>
                <ul><li>Financial Fraud Detection:<br>
                    Analyze 2B transactions/day using GPU-accelerated Zarr pipelines.</li></ul>
                <ul><li>Agricultural Genomics: <br>
                    Process 500,000 rice genome sequences (40TB) with variant-aware chunking.</li></ul>
                <ul><li>Real-Time Language Model Serving: <br>
                    Stream 100GB/hr training updates via Zarr Delta Encoding.</li></ul>
            </p>
        </div>
        <div id="References" class="section">
            <h2>References & Further Reading</h2>
                <li><a href="https://zarr.readthedocs.io/en/stable/">Official Zarr v3 Documentation</a><br>
                <li><a href="https://zarr.dev/blog/zarr-python-3-release/">Zarr-Python 3 release blog</a> <br>
                <li><a href="https://pynwb.readthedocs.io/zarr-tutorial">Cloud native data patterns in Zarr</a> <br>
                <li><a href="https://www.biorxiv.org/content/10.1101/2024.06.11.598241v3.full.pdf">Analysis-ready VCF at Biobank scale using Zarr</a> <br>
            
        </div>

    </div>

    <!-- Footer Section -->
    <footer class="bg-dark text-white text-center py-4 mt-5">
        <p class="mb-0">© 2025 Team J074 | Animesh, Laxmidhar, Bhakti</p>
    </footer>

</body>

</html>